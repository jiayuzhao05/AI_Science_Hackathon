{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c558ea-7f5e-4b4d-a339-7a89e6778290",
   "metadata": {},
   "source": [
    "This jupyter gives you a simple example of how you should use the Simulated Network (asynchronous) environment. This environment is not meant as a training ground of your algorithms, but only to check whether or not your algorithm can be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b654b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\zhaoj\\miniforge3\\lib\\site-packages (from gymnasium) (2.2.4)\n",
      "Collecting cloudpickle>=1.2.0 (from gymnasium)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\zhaoj\\miniforge3\\lib\\site-packages (from gymnasium) (4.13.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "   ---------------------------------------- 0.0/965.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 965.4/965.4 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, cloudpickle, gymnasium\n",
      "Successfully installed cloudpickle-3.1.1 farama-notifications-0.0.4 gymnasium-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92edcf65-93c1-4396-90d0-4572ec09c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "current_dir = Path().resolve()\n",
    "root_dir = current_dir.parent\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.insert(0,str(root_dir))\n",
    "\n",
    "from Gyms.SimulatedNetwork import SimulatedNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a528d5-af3f-4c5b-9bef-0fe0022a21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define size of state and action spaces\n",
    "state_dim  = 4 # Dimension of reduced state space\n",
    "action_dim = 2 # Number of stimuli in action space (each stimulus needs a value of {0,1,2,3,4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5373dba-d31c-4f31-9906-6db506053bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: [0. 0. 0. 0.], Reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Create environment and initialize it\n",
    "env      = SimulatedNetwork(action_dim=action_dim,state_dim=state_dim)\n",
    "state, _ = env.reset()\n",
    "env.render() # This function gives you the current state + reward, which both is 0 after initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b3b3f7-8dc7-455f-bf05-df02d2ef95a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiDiscrete([5 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the action space dimensions\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75680da-4bd1-40b1-a3fe-607e070a5c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (4,), float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the state space dimensions\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc3ef6a-fb75-4834-b6e0-97826997584b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can now for example get a random action:\n",
    "action = env.action_space.sample()\n",
    "action\n",
    "# This action can then be applied to the environment with:\n",
    "# state, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ab4df5-8dc2-4e62-98f8-9fbda33eb750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d5fdf0f-126b-49b9-bef0-79070d77b764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.5       , -0.08075128, -0.42857785,  0.        ]),\n",
       " 4,\n",
       " False,\n",
       " False,\n",
       " {'spikes': array([ 2.88662547,  3.04964471,  7.641619  , 10.01748852, 12.27443785,\n",
       "         13.79736249]),\n",
       "  'elecs': array([1, 2, 3, 0, 1, 2]),\n",
       "  'action': array([3, 1]),\n",
       "  'missed_cyc': 0,\n",
       "  'stim_id': 1,\n",
       "  'simulated': True,\n",
       "  'comment': 'none'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1a4cc22-d672-490f-93c6-d98b3ca82de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulate with action: [0 4]\n",
      "Reward: -1, Avg. reward: -1.0\n",
      "State: [ 0.5        -0.42282242  0.19129197  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 3]\n",
      "Reward: 0, Avg. reward: -0.5\n",
      "State: [ 0.        -0.3030573  0.         0.       ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 4]\n",
      "Reward: 1, Avg. reward: 0.0\n",
      "State: [ 0.5         0.         -0.44462879  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 0]\n",
      "Reward: 0, Avg. reward: 0.0\n",
      "State: [-1.         -0.43428782  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 0]\n",
      "Reward: 0, Avg. reward: 0.0\n",
      "State: [-0.5        -0.47804017  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 2]\n",
      "Reward: 1, Avg. reward: 0.16666666666666666\n",
      "State: [ 0.         -0.40090203  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 0]\n",
      "Reward: 0, Avg. reward: 0.14285714285714285\n",
      "State: [-1.         -0.44515691  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 4]\n",
      "Reward: 0, Avg. reward: 0.125\n",
      "State: [ 0.         -0.10245293  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 3]\n",
      "Reward: 1, Avg. reward: 0.2222222222222222\n",
      "State: [ 0.          0.         -0.21681298  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 3]\n",
      "Reward: 1, Avg. reward: 0.3\n",
      "State: [ 0.          0.         -0.21758616  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 1]\n",
      "Reward: 2, Avg. reward: 0.45454545454545453\n",
      "State: [ 0.5         0.         -0.15606973  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 2]\n",
      "Reward: 1, Avg. reward: 0.5\n",
      "State: [ 0.5         0.         -0.42940468  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 1]\n",
      "Reward: 1, Avg. reward: 0.5384615384615384\n",
      "State: [-1.          0.         -0.11946091  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 0]\n",
      "Reward: 0, Avg. reward: 0.5\n",
      "State: [-0.5  0.   0.   0. ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 4]\n",
      "Reward: 1, Avg. reward: 0.5333333333333333\n",
      "State: [-0.5        0.        -0.4144089  0.       ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 0]\n",
      "Reward: 0, Avg. reward: 0.5\n",
      "State: [0. 0. 0. 0.]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 4]\n",
      "Reward: -1, Avg. reward: 0.4117647058823529\n",
      "State: [-1.          0.41045672  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 2]\n",
      "Reward: 0, Avg. reward: 0.3888888888888889\n",
      "State: [-0.5        -0.32360931  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 1]\n",
      "Reward: -1, Avg. reward: 0.3157894736842105\n",
      "State: [0.         0.         0.23974553 0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 2]\n",
      "Reward: -1, Avg. reward: 0.25\n",
      "State: [0.5 0.  0.  0. ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 0]\n",
      "Reward: 0, Avg. reward: 0.23809523809523808\n",
      "State: [-0.5        -0.45872006  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 0]\n",
      "Reward: 0, Avg. reward: 0.22727272727272727\n",
      "State: [ 0.5        -0.41616158  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 3]\n",
      "Reward: -1, Avg. reward: 0.17391304347826086\n",
      "State: [0.5        0.35767874 0.         0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 3]\n",
      "Reward: -1, Avg. reward: 0.125\n",
      "State: [0.5        0.27724397 0.         0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 4]\n",
      "Reward: -1, Avg. reward: 0.08\n",
      "State: [-1.          0.37456881  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 4]\n",
      "Reward: 0, Avg. reward: 0.07692307692307693\n",
      "State: [ 0.5        -0.45129304  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 1]\n",
      "Reward: 1, Avg. reward: 0.1111111111111111\n",
      "State: [ 0.5        -0.44526636  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 0]\n",
      "Reward: 0, Avg. reward: 0.10714285714285714\n",
      "State: [-0.5        -0.41762463  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 0]\n",
      "Reward: 0, Avg. reward: 0.10344827586206896\n",
      "State: [-0.5        -0.35296212  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 4]\n",
      "Reward: 2, Avg. reward: 0.16666666666666666\n",
      "State: [-0.5        -0.2966749  -0.28134998  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 0]\n",
      "Reward: 0, Avg. reward: 0.16129032258064516\n",
      "State: [-1.         -0.32305915  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 4]\n",
      "Reward: 1, Avg. reward: 0.1875\n",
      "State: [-1.         -0.39571126  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 4]\n",
      "Reward: 1, Avg. reward: 0.21212121212121213\n",
      "State: [-0.5         0.         -0.43451263  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 0]\n",
      "Reward: 0, Avg. reward: 0.20588235294117646\n",
      "State: [ 0.         -0.44749118  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 4]\n",
      "Reward: 2, Avg. reward: 0.2571428571428571\n",
      "State: [ 0.5         0.         -0.30774942  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 3]\n",
      "Reward: 0, Avg. reward: 0.25\n",
      "State: [ 0.5         0.36157223 -0.52200159  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 2]\n",
      "Reward: 1, Avg. reward: 0.2702702702702703\n",
      "State: [-1.         -0.31787896  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 1]\n",
      "Reward: 2, Avg. reward: 0.3157894736842105\n",
      "State: [ 0.         -0.39986852 -0.03867906  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 2]\n",
      "Reward: 1, Avg. reward: 0.3333333333333333\n",
      "State: [-1.         -0.30289056  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 2]\n",
      "Reward: 0, Avg. reward: 0.325\n",
      "State: [-0.5        -0.33121605  0.11614605  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 0]\n",
      "Reward: 0, Avg. reward: 0.3170731707317073\n",
      "State: [-0.5  0.   0.   0. ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 4]\n",
      "Reward: 0, Avg. reward: 0.30952380952380953\n",
      "State: [ 0.5        -0.4327149   0.21632472  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 1]\n",
      "Reward: 0, Avg. reward: 0.3023255813953488\n",
      "State: [-1.         -0.46238045  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 1]\n",
      "Reward: 1, Avg. reward: 0.3181818181818182\n",
      "State: [0.        0.        0.2570344 0.       ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 2]\n",
      "Reward: 0, Avg. reward: 0.3111111111111111\n",
      "State: [-0.5        -0.28528228  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 2]\n",
      "Reward: 2, Avg. reward: 0.34782608695652173\n",
      "State: [ 0.         -0.47405729  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 1]\n",
      "Reward: 1, Avg. reward: 0.3617021276595745\n",
      "State: [ 0.5        -0.39612651  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 1]\n",
      "Reward: 0, Avg. reward: 0.3541666666666667\n",
      "State: [-1.         -0.42280928  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 3]\n",
      "Reward: -1, Avg. reward: 0.32653061224489793\n",
      "State: [ 0.5        -0.49796046  0.0394778   0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 3]\n",
      "Reward: 1, Avg. reward: 0.34\n",
      "State: [-0.5        -0.41371378  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 4]\n",
      "Reward: -1, Avg. reward: 0.3137254901960784\n",
      "State: [-0.5  0.   0.   0. ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 4]\n",
      "Reward: 2, Avg. reward: 0.34615384615384615\n",
      "State: [-1.          0.         -0.18834792  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 2]\n",
      "Reward: 1, Avg. reward: 0.3584905660377358\n",
      "State: [ 0.          0.         -0.34230953  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 3]\n",
      "Reward: 2, Avg. reward: 0.3888888888888889\n",
      "State: [ 0.5         0.         -0.32086739  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 0]\n",
      "Reward: -1, Avg. reward: 0.36363636363636365\n",
      "State: [-0.5         0.          0.25322749  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 2]\n",
      "Reward: -2, Avg. reward: 0.32142857142857145\n",
      "State: [0.         0.29037621 0.         0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 2]\n",
      "Reward: 0, Avg. reward: 0.3157894736842105\n",
      "State: [-0.5         0.         -0.48374123  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 3]\n",
      "Reward: 1, Avg. reward: 0.3275862068965517\n",
      "State: [-0.5        -0.33471962  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 3]\n",
      "Reward: 0, Avg. reward: 0.3220338983050847\n",
      "State: [0.5        0.25614338 0.         0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 0]\n",
      "Reward: 0, Avg. reward: 0.31666666666666665\n",
      "State: [-1.         -0.39452558  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 3]\n",
      "Reward: 1, Avg. reward: 0.32786885245901637\n",
      "State: [ 0.5         0.47006916 -0.35594876  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 2]\n",
      "Reward: 0, Avg. reward: 0.3225806451612903\n",
      "State: [-0.5        -0.36405923  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 4]\n",
      "Reward: 1, Avg. reward: 0.3333333333333333\n",
      "State: [-0.5  0.   0.   0. ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 2]\n",
      "Reward: 1, Avg. reward: 0.34375\n",
      "State: [-1.        -0.1916756  0.         0.       ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 0]\n",
      "Reward: 1, Avg. reward: 0.35384615384615387\n",
      "State: [ 0.5        -0.38641845 -0.23907693  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 3]\n",
      "Reward: 1, Avg. reward: 0.36363636363636365\n",
      "State: [ 0.          0.         -0.43647192  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 3]\n",
      "Reward: -1, Avg. reward: 0.34328358208955223\n",
      "State: [0.5        0.29522112 0.         0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 3]\n",
      "Reward: 0, Avg. reward: 0.3382352941176471\n",
      "State: [-1.         -0.43752409 -0.12108823  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 1]\n",
      "Reward: 0, Avg. reward: 0.3333333333333333\n",
      "State: [-1.       -0.383771  0.        0.      ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 4]\n",
      "Reward: 1, Avg. reward: 0.34285714285714286\n",
      "State: [ 0.5         0.         -0.22839796  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 0]\n",
      "Reward: -1, Avg. reward: 0.323943661971831\n",
      "State: [0.         0.         0.19330652 0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 1]\n",
      "Reward: 0, Avg. reward: 0.3194444444444444\n",
      "State: [-0.5         0.33451544  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 4]\n",
      "Reward: 0, Avg. reward: 0.3150684931506849\n",
      "State: [ 0.         -0.34438117  0.11225099  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 4]\n",
      "Reward: 2, Avg. reward: 0.33783783783783783\n",
      "State: [-1.          0.         -0.14483742  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 1]\n",
      "Reward: 2, Avg. reward: 0.36\n",
      "State: [ 0.         -0.33930984 -0.21920731  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 2]\n",
      "Reward: 1, Avg. reward: 0.3684210526315789\n",
      "State: [-1.         -0.31132328  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 0]\n",
      "Reward: 0, Avg. reward: 0.36363636363636365\n",
      "State: [-0.5        -0.44450069  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 2]\n",
      "Reward: 1, Avg. reward: 0.3717948717948718\n",
      "State: [ 0.5         0.         -0.24765597  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 2]\n",
      "Reward: 0, Avg. reward: 0.3670886075949367\n",
      "State: [-0.5        -0.40388754  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 4]\n",
      "Reward: 0, Avg. reward: 0.3625\n",
      "State: [0.         0.18823588 0.         0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 4]\n",
      "Reward: 2, Avg. reward: 0.38271604938271603\n",
      "State: [-1.          0.         -0.35724029  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 2]\n",
      "Reward: 0, Avg. reward: 0.3780487804878049\n",
      "State: [-0.5        -0.30409068  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 3]\n",
      "Reward: -2, Avg. reward: 0.3493975903614458\n",
      "State: [0.5        0.26604315 0.         0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 4]\n",
      "Reward: -1, Avg. reward: 0.3333333333333333\n",
      "State: [-0.5         0.          0.13576369  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 1]\n",
      "Reward: 2, Avg. reward: 0.35294117647058826\n",
      "State: [ 0.         -0.39637527 -0.12151195  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 1]\n",
      "Reward: -1, Avg. reward: 0.3372093023255814\n",
      "State: [0.         0.         0.20308197 0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [1 1]\n",
      "Reward: 0, Avg. reward: 0.3333333333333333\n",
      "State: [-1.         -0.45454433  0.20669636  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 3]\n",
      "Reward: 2, Avg. reward: 0.3522727272727273\n",
      "State: [ 0.5         0.         -0.25092479  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [3 2]\n",
      "Reward: -2, Avg. reward: 0.3258426966292135\n",
      "State: [ 0.         -0.38291305  0.14707254  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 0]\n",
      "Reward: 0, Avg. reward: 0.32222222222222224\n",
      "State: [-0.5      -0.449748  0.        0.      ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 1]\n",
      "Reward: 1, Avg. reward: 0.32967032967032966\n",
      "State: [-1.         -0.33151237 -0.13781395  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 4]\n",
      "Reward: 2, Avg. reward: 0.34782608695652173\n",
      "State: [-0.5        -0.47434129 -0.08757428  0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 1]\n",
      "Reward: -1, Avg. reward: 0.3333333333333333\n",
      "State: [-0.5         0.51874454  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 1]\n",
      "Reward: 0, Avg. reward: 0.32978723404255317\n",
      "State: [-1.         -0.33385593  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [4 1]\n",
      "Reward: 1, Avg. reward: 0.3368421052631579\n",
      "State: [ 0.5        -0.45542701  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 1]\n",
      "Reward: 1, Avg. reward: 0.34375\n",
      "State: [-0.5        -0.44486382  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 3]\n",
      "Reward: 0, Avg. reward: 0.3402061855670103\n",
      "State: [ 0.         -0.44767592  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 3]\n",
      "Reward: 0, Avg. reward: 0.336734693877551\n",
      "State: [ 0.         -0.40177454  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [0 4]\n",
      "Reward: 0, Avg. reward: 0.3333333333333333\n",
      "State: [ 0.5        -0.40917168  0.          0.        ]\n",
      "-----------------------------\n",
      "Stimulate with action: [2 1]\n",
      "Reward: 1, Avg. reward: 0.34\n",
      "State: [-0.5        -0.42656159  0.          0.        ]\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example code, that stimulates the network 100 times with a randomly sampled action, while calculating also the average reward received\n",
    "\n",
    "total_reward = 0\n",
    "action_count = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    # For simplicity, choose a random action\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"Stimulate with action: {action}\")\n",
    "    \n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    action_count += 1\n",
    "\n",
    "    print(f\"Reward: {reward}, Avg. reward: {total_reward/action_count}\")\n",
    "    print(f\"State: {state}\")\n",
    "\n",
    "    # If you want a more complete plotting of each step\n",
    "    # env.render()\n",
    "\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e01d04d-21a2-4760-9f81-7bfe2884a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08d6f566-df73-4654-8d63-72bded5f1fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5       , 0.        , 0.37476053, 0.        ]), -2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state,reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88227413-b0c8-42a3-be75-f3098e654f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spikes': array([ 0.82112395,  4.50709643,  8.01823285,  8.09481998, 10.91126747]),\n",
       " 'elecs': array([3, 1, 0, 2, 1]),\n",
       " 'action': array([2, 1]),\n",
       " 'missed_cyc': 0,\n",
       " 'stim_id': 102,\n",
       " 'simulated': True,\n",
       " 'comment': 'none'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9908809f-ad71-4ed0-9d67-9ce4eeaabe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.34\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average reward: {total_reward/action_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
